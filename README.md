.# ğŸ¤Ÿ Sign Language to Speech Translator ğŸ—£ï¸  
Translate sign language gestures (using both hands) into real-time speech using computer vision and deep learning.

Built with â¤ï¸ using **TensorFlow**, **MediaPipe**, **OpenCV**, and **pyttsx3**.

---

## ğŸ“ Project Structure

```
SIGN_LANGUAGE_TO_SPEECH/
â”œâ”€â”€ accuracy/
â”‚   â”œâ”€â”€ accuracy_loss.png           # Training performance graph
â”‚   â”œâ”€â”€ test.txt                    # List/log of test samples
â”‚   â””â”€â”€ train.txt                   # List/log of training samples
â”œâ”€â”€ data/
â”‚   â””â”€â”€ gesture_landmarks_both_hands.csv  # Collected landmarks dataset
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ gesture_dl_model.keras     # Trained gesture recognition model
â”‚   â””â”€â”€ label_encoder.pkl          # Encoded gesture labels
â”œâ”€â”€ total_signs/
â”‚   â”œâ”€â”€ display_signs.py           # Display supported gestures
â”‚   â””â”€â”€ toal_signs.txt             # All gesture labels list
â”œâ”€â”€ app.py                         # Real-time sign-to-speech translator
â”œâ”€â”€ collect_data.py                # Gesture data collection script
â”œâ”€â”€ train_dl_model.py              # Model training script
â”œâ”€â”€ LICENSE                        # Project license
â””â”€â”€ README.md                      # Project documentation
```

---

## ğŸš€ Project Overview

This system enables real-time **gesture-based communication** by recognizing sign language gestures and converting them into **spoken English** using speech synthesis.  
Itâ€™s ideal for:
- People with speech/hearing impairments
- Human-computer interaction (HCI)
- Educational demos and assistive tech

---

## ğŸ”§ Setup & Requirements

Install all required libraries:

```bash
pip install tensorflow mediapipe opencv-python numpy pandas matplotlib pyttsx3 scikit-learn
```

---

## ğŸ¯ How to Use

### 1ï¸âƒ£ Collect Gesture Data

```bash
python collect_data.py
```
- Input the gesture label (e.g., "hello", "thank you").
- Perform the gesture in front of the webcam.
- The script captures 3D landmark data of both hands.

### 2ï¸âƒ£ Train the Model

```bash
python train_dl_model.py
```
- Trains a Dense Neural Network on the collected gestures.
- Saves model (`gesture_dl_model.keras`) and encoder (`label_encoder.pkl`).
- Also visualizes training loss and accuracy.

### 3ï¸âƒ£ Run Real-time Translator

```bash
python app.py
```
- Detects both hands using MediaPipe.
- Captures movement for 2 seconds once hands are detected.
- Predicts the most likely gesture.
- Uses `pyttsx3` to speak out the recognized sign.

---

## ğŸ“Š Model Performance

The graph below shows accuracy and loss trends during training:

![Accuracy & Loss Graph](accuracy/accuracy_loss.png)

---

## ğŸ“ Adding New Gestures

1. Collect new gesture samples using `collect_data.py`.
2. Append the new data to `gesture_landmarks_both_hands.csv`.
3. Retrain the model with `train_dl_model.py`.
4. Update `toal_signs.txt` with new labels.

---

## ğŸ“¦ Files to Note

| File | Description |
|------|-------------|
| `app.py` | Real-time webcam-based gesture recognition & speech |
| `collect_data.py` | Capture hand gesture data samples |
| `train_dl_model.py` | Train and evaluate gesture classification model |
| `model/` | Stores the saved DL model & label encoder |
| `data/` | Stores collected gesture data (CSV format) |

---

## ğŸ’¡ Future Improvements

- Add support for continuous sentence-level translation  
- Integrate sign-to-text + text-to-speech pipelines  
- Build a user-friendly desktop or mobile interface  
- Add more sign languages (BSL, ISL, etc.)

---

## ğŸ‘¨â€ğŸ’» Author

**Divya Khunt**  
B.Tech in Engineering | ML & Deep Learning Enthusiast  
[GitHub Profile](#)

---

## ğŸ“„ License

This project is licensed under the [MIT License](LICENSE).

---

## ğŸŒŸ Show Some Love

If you found this project helpful:
- â­ Star the repo
- ğŸ§  Share it with friends
- ğŸ” Fork it and build on top of it!
