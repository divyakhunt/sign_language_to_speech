# ğŸ¤Ÿ Sign Language to Speech Translator ğŸ—£ï¸  
Translate sign language gestures (using both hands) into real-time speech using computer vision and deep learning.

Built with â¤ï¸ using **TensorFlow**, **MediaPipe**, **OpenCV**, and **pyttsx3**.

---

## ğŸ“ Project Structure

```
SIGN_LANGUAGE_TO_SPEECH/
â”œâ”€â”€ accuracy/
â”‚   â”œâ”€â”€ accuracy_loss.png            # Training accuracy/loss plot
â”‚   â”œâ”€â”€ test.txt                     # Test log or label list
â”‚   â””â”€â”€ train.txt                    # Training log or label list
â”œâ”€â”€ data/
â”‚   â””â”€â”€ gesture_landmarks_both_hands.csv  # Collected hand gesture landmarks
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ gesture_dl_model.keras      # Trained DL model
â”‚   â””â”€â”€ label_encoder.pkl           # Label encoder for gestures
â”œâ”€â”€ total_signs/
â”‚   â”œâ”€â”€ display_signs.py            # Script to display supported signs
â”‚   â””â”€â”€ toal_signs.txt              # Text file with sign labels
â”œâ”€â”€ app.py                          # Main sign-to-speech inference script
â”œâ”€â”€ collect_data.py                 # Data collection script for gestures
â”œâ”€â”€ train_dl_model.py               # Model training script
â”œâ”€â”€ LICENSE                         # License file
â””â”€â”€ README.md                       # Project documentation
```

---

## ğŸ› ï¸ How It Works

1. **Data Collection (`collect_data.py`)**  
   Capture gesture landmark coordinates for both hands using MediaPipe and store them in a CSV file.

2. **Model Training (`train_dl_model.py`)**  
   Trains a deep learning classifier (Dense NN) on the gesture landmark data and saves the model and label encoder.

3. **Real-time Inference (`app.py`)**  
   Detects gestures live from the webcam, classifies them using the trained model, and speaks out the recognized gesture using `pyttsx3`.

---

## ğŸ“¦ Requirements

Install the required Python libraries:

```bash
pip install tensorflow mediapipe opencv-python numpy pandas matplotlib pyttsx3 scikit-learn
```

---

## ğŸš€ Quick Start

### ğŸ”¹ 1. Collect Gesture Data

```bash
python collect_data.py
```
Follow the prompts to record samples for a gesture. Make sure both hands are visible.

### ğŸ”¹ 2. Train the Model

```bash
python train_dl_model.py
```
This trains the model and saves it under the `model/` directory.

### ğŸ”¹ 3. Run the Translator

```bash
python app.py
```
The app waits for a gesture, records for 2 seconds, predicts the gesture, and converts it to speech.

---

## ğŸ“Š Visualization

![Training Accuracy and Loss](accuracy/accuracy_loss.png)

---

## ğŸ“‚ Add/Update Supported Gestures

1. Add more samples using `collect_data.py`.
2. Retrain the model with `train_dl_model.py`.
3. Keep `toal_signs.txt` updated with the list of gestures.

---

## ğŸ—£ï¸ Example Use Cases

- Accessibility aid for speech-impaired individuals  
- Educational tool for learning sign language  
- Real-time communication bridge using gesture recognition

---

## ğŸ“„ License

This project is licensed under the [MIT License](LICENSE).

---

## ğŸ™‹â€â™€ï¸ Author

**Divya Khunt**  
ML & DL Enthusiast | B.Tech in Engineering

---

## ğŸŒŸ Show your support

If you like this project, leave a â­ on [GitHub](#) and share it with others!
